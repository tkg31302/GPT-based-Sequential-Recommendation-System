{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gzip\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.sparse import coo_matrix\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "try:\n",
        "    import implicit\n",
        "except ImportError:\n",
        "    implicit = None"
      ],
      "metadata": {
        "id": "VbmjgJerFgQQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfYJNz9pFmgZ",
        "outputId": "71cbb346-d4fa-4ade-b943-bf0e48a74f34"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lHEKIBGpE0_f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eTcNCrdsT7Sn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# SASRec: Self-Attentive Sequential Recommendation\n",
        "# Paper: https://arxiv.org/pdf/1808.09781\n",
        "# ============================================================\n",
        "\n",
        "import math\n",
        "import random\n",
        "import pickle\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ============================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================\n",
        "\n",
        "DATA_DIR = \"/content/drive/MyDrive/data/\"\n",
        "TRAIN_PKL = DATA_DIR + \"train_data.pkl\"\n",
        "VAL_PKL   = DATA_DIR + \"val_data.pkl\"\n",
        "TEST_PKL  = DATA_DIR + \"test_data.pkl\"\n",
        "\n",
        "# Model hyperparameters\n",
        "MAX_LEN = 50\n",
        "HIDDEN_UNITS = 50\n",
        "HEADS = 1\n",
        "LAYERS = 2\n",
        "DROPOUT = 0.2\n",
        "\n",
        "# Training settings\n",
        "BATCH_SIZE = 128\n",
        "NEG_SAMPLES = 100\n",
        "EPOCHS = 5\n",
        "LR = 0.001\n",
        "TOPK = 10\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"[INFO] Device:\", device)\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# DATA LOADING\n",
        "# ============================================================\n",
        "\n",
        "def load_pickle_data(pkl_path):\n",
        "    \"\"\"Load pickle file and convert to (sequences, targets, lengths) format.\"\"\"\n",
        "    if not os.path.exists(pkl_path):\n",
        "        raise FileNotFoundError(f\"Pickle file not found: {pkl_path}\")\n",
        "\n",
        "    with open(pkl_path, \"rb\") as f:\n",
        "        data = pickle.load(f)\n",
        "\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    lengths = []\n",
        "\n",
        "    for user_id, user_data in data.items():\n",
        "        if 'sequence' in user_data and len(user_data['sequence']) > 0:\n",
        "            seq = [item['item_id'] for item in user_data['sequence']]\n",
        "\n",
        "            # Handle validation/test data with separate target field\n",
        "            if 'target' in user_data:\n",
        "                target = user_data['target']['item_id'] if isinstance(user_data['target'], dict) else user_data['target']\n",
        "                input_seq = [item for item in seq if item != target]\n",
        "                sequences.append(input_seq)\n",
        "                targets.append(target)\n",
        "                lengths.append(len(input_seq))\n",
        "            else:\n",
        "                # Training data: target is last item in sequence\n",
        "                if len(seq) > 1:\n",
        "                    sequences.append(seq[:-1])\n",
        "                    targets.append(seq[-1])\n",
        "                    lengths.append(len(seq) - 1)\n",
        "                elif len(seq) == 1:\n",
        "                    sequences.append([])\n",
        "                    targets.append(seq[0])\n",
        "                    lengths.append(0)\n",
        "\n",
        "    return sequences, targets, lengths\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# DATASET\n",
        "# ============================================================\n",
        "\n",
        "class PickleDataset(Dataset):\n",
        "    def __init__(self, pkl_path, max_len):\n",
        "        self.seqs, self.targets, self.lengths = load_pickle_data(pkl_path)\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.seqs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.seqs[idx]\n",
        "        tgt = self.targets[idx]\n",
        "\n",
        "        # Truncate to max_len and left-pad with zeros\n",
        "        seq = seq[-self.max_len:]\n",
        "        seq = [0] * (self.max_len - len(seq)) + seq\n",
        "\n",
        "        return torch.tensor(seq, dtype=torch.long), torch.tensor(tgt)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# INITIALIZE DATA LOADERS\n",
        "# ============================================================\n",
        "\n",
        "# Calculate vocab size from all splits to ensure coverage\n",
        "tr_seqs, tr_targets, _ = load_pickle_data(TRAIN_PKL)\n",
        "val_seqs, val_targets, _ = load_pickle_data(VAL_PKL)\n",
        "test_seqs, test_targets, _ = load_pickle_data(TEST_PKL)\n",
        "\n",
        "all_seqs = tr_seqs + val_seqs + test_seqs\n",
        "all_targets = tr_targets + val_targets + test_targets\n",
        "\n",
        "max_seq_item = max([max(seq) for seq in all_seqs if len(seq) > 0]) if all_seqs else 0\n",
        "max_target = max(all_targets) if all_targets else 0\n",
        "VOCAB_SIZE = max(max_seq_item, max_target) + 1\n",
        "print(\"[INFO] Vocab size:\", VOCAB_SIZE)\n",
        "\n",
        "train_loader = DataLoader(PickleDataset(TRAIN_PKL, MAX_LEN), batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(PickleDataset(VAL_PKL, MAX_LEN), batch_size=1)\n",
        "test_loader = DataLoader(PickleDataset(TEST_PKL, MAX_LEN), batch_size=1)\n",
        "\n",
        "print(\"[INFO] Train samples:\", len(train_loader.dataset))\n",
        "print(\"[INFO] Val samples:\", len(val_loader.dataset))\n",
        "print(\"[INFO] Test samples:\", len(test_loader.dataset))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SASRec MODEL\n",
        "# ============================================================\n",
        "\n",
        "class SASRec(nn.Module):\n",
        "    def __init__(self, item_num):\n",
        "        super().__init__()\n",
        "        self.item_emb = nn.Embedding(item_num, HIDDEN_UNITS, padding_idx=0)\n",
        "        self.pos_emb = nn.Embedding(MAX_LEN, HIDDEN_UNITS)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=HIDDEN_UNITS,\n",
        "            nhead=HEADS,\n",
        "            dim_feedforward=HIDDEN_UNITS * 4,\n",
        "            dropout=DROPOUT,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, LAYERS)\n",
        "\n",
        "        # Initialize item embeddings\n",
        "        nn.init.normal_(self.item_emb.weight, mean=0.0, std=HIDDEN_UNITS ** -0.5)\n",
        "\n",
        "    def forward(self, seq):\n",
        "        B, T = seq.shape\n",
        "        seq = torch.clamp(seq, 0, self.item_emb.num_embeddings - 1)\n",
        "        pos_ids = torch.arange(T, device=seq.device).unsqueeze(0)\n",
        "        x = self.item_emb(seq) + self.pos_emb(pos_ids)\n",
        "        mask = (seq == 0)\n",
        "        h = self.encoder(x, src_key_padding_mask=mask)\n",
        "        return h[:, -1]\n",
        "\n",
        "\n",
        "model = SASRec(VOCAB_SIZE).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# BPR LOSS\n",
        "# ============================================================\n",
        "\n",
        "def bpr_loss(h, pos, neg):\n",
        "    \"\"\"Bayesian Personalized Ranking loss.\"\"\"\n",
        "    pos_emb = model.item_emb(pos)\n",
        "    neg_emb = model.item_emb(neg)\n",
        "    pos_score = torch.sum(h * pos_emb, dim=-1)\n",
        "    neg_score = torch.sum(h.unsqueeze(1) * neg_emb, dim=-1)\n",
        "    return -torch.log(torch.sigmoid(pos_score.unsqueeze(1) - neg_score) + 1e-8).mean()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# EVALUATION\n",
        "# ============================================================\n",
        "\n",
        "def ndcg(rank):\n",
        "    \"\"\"Normalized Discounted Cumulative Gain.\"\"\"\n",
        "    return 1 / math.log2(rank + 1)\n",
        "\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    \"\"\"Evaluate model on validation/test set.\"\"\"\n",
        "    model.eval()\n",
        "    hits = 0\n",
        "    mrr = 0\n",
        "    ndcg_sum = 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for seq, tgt in loader:\n",
        "            seq = seq.to(device)\n",
        "            tgt = tgt.item()\n",
        "\n",
        "            # Skip if target is out of vocabulary range\n",
        "            if tgt >= VOCAB_SIZE or tgt < 1:\n",
        "                count += 1\n",
        "                continue\n",
        "\n",
        "            seq_list = seq.squeeze(0).tolist()\n",
        "            user_items = set([item for item in seq_list if item != 0 and item < VOCAB_SIZE])\n",
        "            if tgt in user_items:\n",
        "                user_items.discard(tgt)\n",
        "\n",
        "            # Sample 100 negatives not in user's history\n",
        "            negatives = set()\n",
        "            attempts = 0\n",
        "            while len(negatives) < 100 and attempts < 10000:\n",
        "                neg = random.randint(1, VOCAB_SIZE - 1)\n",
        "                if neg != tgt and neg not in user_items:\n",
        "                    negatives.add(neg)\n",
        "                attempts += 1\n",
        "\n",
        "            # Rank target among candidates (all should be valid now)\n",
        "            candidates = [tgt] + list(negatives)\n",
        "            random.shuffle(candidates)\n",
        "\n",
        "            h = model(seq)\n",
        "            cand_tensor = torch.tensor(candidates, device=device, dtype=torch.long)\n",
        "\n",
        "            # Validate all candidates are in range before embedding\n",
        "            if cand_tensor.max() >= VOCAB_SIZE or cand_tensor.min() < 0:\n",
        "                count += 1\n",
        "                continue\n",
        "\n",
        "            cand_emb = model.item_emb(cand_tensor)\n",
        "            scores = torch.matmul(h, cand_emb.T).squeeze(0)\n",
        "            rankings = torch.argsort(scores, descending=True)\n",
        "            ranked_items = [candidates[i] for i in rankings.tolist()]\n",
        "            rank = ranked_items.index(tgt) + 1\n",
        "\n",
        "            if rank <= TOPK:\n",
        "                hits += 1\n",
        "                mrr += 1 / rank\n",
        "                ndcg_sum += ndcg(rank)\n",
        "            count += 1\n",
        "\n",
        "    hr = hits / count if count > 0 else 0.0\n",
        "    mrr_score = mrr / count if count > 0 else 0.0\n",
        "    ndcg_score = ndcg_sum / count if count > 0 else 0.0\n",
        "    return hr, mrr_score, ndcg_score\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TRAINING\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n[INFO] Training started\\n\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    steps = 0\n",
        "\n",
        "    for seq, tgt in train_loader:\n",
        "        seq = seq.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "        B = seq.shape[0]\n",
        "\n",
        "        # Sample random negatives\n",
        "        neg = torch.randint(1, VOCAB_SIZE, (B, NEG_SAMPLES), device=device)\n",
        "\n",
        "        # Forward pass\n",
        "        h = model(seq)\n",
        "        loss = bpr_loss(h, tgt, neg)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        steps += 1\n",
        "\n",
        "    print(f\"[TRAIN] Epoch {epoch+1}  Avg Loss = {total_loss/steps:.4f}\")\n",
        "\n",
        "    # Validation evaluation\n",
        "    hr, mrr_score, ndcg_score = evaluate(model, val_loader)\n",
        "    print(f\"[VAL]   Epoch {epoch+1}  HR@10={hr:.4f}  MRR@10={mrr_score:.4f}  NDCG@10={ndcg_score:.4f}\\n\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# FINAL TEST RESULTS\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n[INFO] Final Test Results\")\n",
        "hr, mrr_score, ndcg_score = evaluate(model, test_loader)\n",
        "print(f\"[TEST]  HR@10={hr:.4f}  MRR@10={mrr_score:.4f}  NDCG@10={ndcg_score:.4f}\")\n",
        "\n",
        "print(\"\\n========== DONE ==========\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyWM3hlltyuz",
        "outputId": "313a7493-b30c-433c-bd25-f1afb43d1602"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Device: cuda\n",
            "[INFO] Vocab size: 389162\n",
            "[INFO] Train samples: 100000\n",
            "[INFO] Val samples: 100000\n",
            "[INFO] Test samples: 100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] Training started\n",
            "\n",
            "[TRAIN] Epoch 1  Avg Loss = 0.6690\n",
            "[VAL]   Epoch 1  HR@10=0.3872  MRR@10=0.2104  NDCG@10=0.2520\n",
            "\n",
            "[TRAIN] Epoch 2  Avg Loss = 0.2121\n",
            "[VAL]   Epoch 2  HR@10=0.4052  MRR@10=0.2377  NDCG@10=0.2773\n",
            "\n",
            "[TRAIN] Epoch 3  Avg Loss = 0.1155\n",
            "[VAL]   Epoch 3  HR@10=0.4064  MRR@10=0.2392  NDCG@10=0.2787\n",
            "\n",
            "[TRAIN] Epoch 4  Avg Loss = 0.0623\n",
            "[VAL]   Epoch 4  HR@10=0.4044  MRR@10=0.2311  NDCG@10=0.2719\n",
            "\n",
            "[TRAIN] Epoch 5  Avg Loss = 0.0281\n",
            "[VAL]   Epoch 5  HR@10=0.3970  MRR@10=0.2197  NDCG@10=0.2614\n",
            "\n",
            "\n",
            "[INFO] Final Test Results\n",
            "[TEST]  HR@10=0.3803  MRR@10=0.2030  NDCG@10=0.2446\n",
            "\n",
            "========== DONE ==========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XOUuwd-L7Fde"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}