{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e026ca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f1e47f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle('/data/sukhanna/cse258/processed_100/train_data.pkl')\n",
    "val_data = pd.read_pickle('/data/sukhanna/cse258/processed_100/val_data.pkl')\n",
    "test_data = pd.read_pickle('/data/sukhanna/cse258/processed_100/test_data.pkl')\n",
    "\n",
    "id2item = pd.read_pickle('/data/sukhanna/cse258/processed_100/id2item.pkl')\n",
    "item2id = pd.read_pickle('/data/sukhanna/cse258/processed_100/item2id.pkl')\n",
    "\n",
    "id2user = pd.read_pickle('/data/sukhanna/cse258/processed_100/id2user.pkl')\n",
    "user2id = pd.read_pickle('/data/sukhanna/cse258/processed_100/user2id.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c0c24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': [{'item_id': 239726, 'timestamp': 1373155244.0, 'rating': 4.0},\n",
       "  {'item_id': 100927, 'timestamp': 1398238497.0, 'rating': 5.0},\n",
       "  {'item_id': 239727, 'timestamp': 1447851667.0, 'rating': 5.0}],\n",
       " 'is_autoregressive': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[8551]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8394909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': [{'item_id': 239726, 'timestamp': 1373155244.0, 'rating': 4.0},\n",
       "  {'item_id': 100927, 'timestamp': 1398238497.0, 'rating': 5.0},\n",
       "  {'item_id': 239727, 'timestamp': 1447851667.0, 'rating': 5.0}],\n",
       " 'target': {'item_id': 239728, 'timestamp': 1448406034.0, 'rating': 4.0},\n",
       " 'is_autoregressive': False}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[8551]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e5108c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': [{'item_id': 239726, 'timestamp': 1373155244.0, 'rating': 4.0},\n",
       "  {'item_id': 100927, 'timestamp': 1398238497.0, 'rating': 5.0},\n",
       "  {'item_id': 239727, 'timestamp': 1447851667.0, 'rating': 5.0},\n",
       "  {'item_id': 239728, 'timestamp': 1448406034.0, 'rating': 4.0}],\n",
       " 'target': {'item_id': 135537, 'timestamp': 1569107616.096, 'rating': 4.0},\n",
       " 'is_autoregressive': False}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[8551]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e92df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class BPRDataset(Dataset):\n",
    "    def __init__(self, train_data, val_data, test_data, num_items):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train_data: List of dicts containing 'sequence' (list of dicts with 'item_id').\n",
    "            val_data: List of dicts containing 'target' ('item_id').\n",
    "            test_data: List of dicts containing 'target' ('item_id').\n",
    "            num_items: Total count of items in catalog (e.g., 349,000).\n",
    "                       Assumes IDs range from 1 to num_items.\n",
    "        \"\"\"\n",
    "        self.num_items = num_items\n",
    "        \n",
    "        # Pre-process data into simple sets for O(1) lookups\n",
    "        self.user_history = []\n",
    "        self.exclusion_sets = []\n",
    "        self.valid_user_indices = []\n",
    "\n",
    "        # Assuming train_data, val_data, test_data are aligned by list index (User ID)\n",
    "        for u_idx in range(len(train_data)):\n",
    "            # 1. Extract Train Sequence\n",
    "            # The data structure is nested: entry['sequence'] -> list of dicts -> 'item_id'\n",
    "            train_seq = [x['item_id'] for x in train_data[u_idx]['sequence']]\n",
    "            \n",
    "            # 2. Extract Validation and Test Targets\n",
    "            val_target = val_data[u_idx]['target']['item_id']\n",
    "            test_target = test_data[u_idx]['target']['item_id']\n",
    "            \n",
    "            # 3. Store Positive History for Training (only train_seq matters for BPR positive sampling)\n",
    "            # We filter out users with empty sequences to prevent errors\n",
    "            if len(train_seq) > 0:\n",
    "                self.user_history.append(train_seq)\n",
    "                \n",
    "                # 4. Build Exclusion Set (Train + Val + Test)\n",
    "                # These are items we CANNOT use as negatives\n",
    "                exclude = set(train_seq)\n",
    "                exclude.add(val_target)\n",
    "                exclude.add(test_target)\n",
    "                self.exclusion_sets.append(exclude)\n",
    "                \n",
    "                # Keep track of original user index if needed, though BPR usually learns \n",
    "                # user_id based on the row index of this dataset\n",
    "                self.valid_user_indices.append(u_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_history)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            user_id: (0 to N-1)\n",
    "            pos_item: (0 to M-1)\n",
    "            neg_item: (0 to M-1)\n",
    "        \"\"\"\n",
    "        # 1. User ID (mapped to 0-index)\n",
    "        # We use the index of the dataset as the user_id for embedding lookup\n",
    "        user_id = idx \n",
    "        \n",
    "        # 2. Positive Sampling\n",
    "        # Randomly select one item from the user's training history\n",
    "        pos_id_raw = random.choice(self.user_history[idx])\n",
    "        \n",
    "        # 3. Negative Sampling with Exclusion\n",
    "        # Randomly sample until we find an item NOT in the exclusion set\n",
    "        while True:\n",
    "            # Sample from 1 to num_items (inclusive)\n",
    "            neg_id_raw = random.randint(1, self.num_items)\n",
    "            \n",
    "            if neg_id_raw not in self.exclusion_sets[idx]:\n",
    "                break\n",
    "        \n",
    "        # 4. Convert to 0-based indexing for PyTorch Embedding Layers\n",
    "        # Input IDs are 1-based, so we subtract 1.\n",
    "        return (\n",
    "            torch.tensor(user_id, dtype=torch.long),\n",
    "            torch.tensor(pos_id_raw - 1, dtype=torch.long),\n",
    "            torch.tensor(neg_id_raw - 1, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "# class BPRDataset(Dataset):\n",
    "#     def __init__(self, train_data, val_data, test_data, num_items, n_neg=1): # <--- Added n_neg\n",
    "#         # ... (previous init code is same) ...\n",
    "#         self.n_neg = n_neg # Store it\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         user_id = idx \n",
    "#         pos_id_raw = random.choice(self.user_history[idx])\n",
    "        \n",
    "#         # --- MODIFIED BLOCK START ---\n",
    "#         neg_samples = []\n",
    "#         for _ in range(self.n_neg): # Loop N times\n",
    "#             while True:\n",
    "#                 neg_id_raw = random.randint(1, self.num_items)\n",
    "#                 if neg_id_raw not in self.exclusion_sets[idx]:\n",
    "#                     neg_samples.append(neg_id_raw - 1)\n",
    "#                     break\n",
    "#         # --- MODIFIED BLOCK END ---\n",
    "\n",
    "#         return (\n",
    "#             torch.tensor(user_id, dtype=torch.long),\n",
    "#             torch.tensor(pos_id_raw - 1, dtype=torch.long),\n",
    "#             torch.tensor(neg_samples, dtype=torch.long) # Shape: [n_neg]\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d9f2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BPRMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_users: Total number of unique users.\n",
    "            num_items: Total number of unique items.\n",
    "            embedding_dim: Size of the latent vectors (e.g., 32, 64, 128).\n",
    "        \"\"\"\n",
    "        super(BPRMF, self).__init__()\n",
    "        \n",
    "        # 1. User Embeddings\n",
    "        # Shape: [num_users, embedding_dim]\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        \n",
    "        # 2. Item Embeddings\n",
    "        # Shape: [num_items, embedding_dim]\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # Initialization (Critical for BPR convergence)\n",
    "        # We initialize with small random values (Normal distribution)\n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_embedding.weight, std=0.01)\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        \"\"\"\n",
    "        Computes the compatibility score (dot product) between users and items.\n",
    "        \n",
    "        Args:\n",
    "            user_indices: Tensor of shape [batch_size]\n",
    "            item_indices: Tensor of shape [batch_size]\n",
    "            \n",
    "        Returns:\n",
    "            scores: Tensor of shape [batch_size]\n",
    "        \"\"\"\n",
    "        # Look up latent vectors\n",
    "        # user_vec: [batch_size, embedding_dim]\n",
    "        # item_vec: [batch_size, embedding_dim]\n",
    "        user_vec = self.user_embedding(user_indices)\n",
    "        item_vec = self.item_embedding(item_indices)\n",
    "        \n",
    "        # Compute Dot Product\n",
    "        # Multiply element-wise and sum across the embedding dimension (dim=1)\n",
    "        scores = (user_vec * item_vec).sum(dim=1)\n",
    "        \n",
    "        return scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08e4a314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "class BPREvaluator:\n",
    "    def __init__(self, eval_data, exclusion_rules, num_items, k_list=[5, 10]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            eval_data: List of dicts (Validation or Test data).\n",
    "            exclusion_rules: Dict {user_idx: set(all_positive_items)}.\n",
    "            num_items: Total catalog size (for random negative sampling).\n",
    "            k_list: List of K values for metrics (e.g., [5, 10]).\n",
    "        \"\"\"\n",
    "        self.eval_data = eval_data\n",
    "        self.exclusion_rules = exclusion_rules\n",
    "        self.num_items = num_items\n",
    "        self.k_list = k_list\n",
    "    \n",
    "    def evaluate(self, model, device='cpu'):\n",
    "        model.eval()  # Switch model to evaluation mode\n",
    "        \n",
    "        # Accumulators for metrics\n",
    "        hr_results = {k: [] for k in self.k_list}\n",
    "        ndcg_results = {k: [] for k in self.k_list}\n",
    "        \n",
    "        # We iterate through each user in the evaluation set\n",
    "        with torch.no_grad():\n",
    "            for u_idx, entry in self.eval_data.items():\n",
    "                \n",
    "                # 1. Get Ground Truth (Target)\n",
    "                # Ensure we handle the 1-based indexing -> 0-based conversion\n",
    "                gt_item_raw = entry['target']['item_id']\n",
    "                gt_item = gt_item_raw - 1\n",
    "                \n",
    "                # 2. Sample 100 Negatives\n",
    "                # These must NOT be in the exclusion set (Train + Val + Test)\n",
    "                negatives = []\n",
    "                u_exclusion = self.exclusion_rules[u_idx]\n",
    "                \n",
    "                while len(negatives) < 100:\n",
    "                    neg_candidate = random.randint(1, self.num_items)\n",
    "                    \n",
    "                    # Check exclusion and duplication within the current batch of 100\n",
    "                    if (neg_candidate not in u_exclusion) and (neg_candidate - 1 != gt_item):\n",
    "                        # Add to list (converting to 0-based index)\n",
    "                        negatives.append(neg_candidate - 1)\n",
    "                \n",
    "                # 3. Prepare Batch for Model (1 GT + 100 Negatives)\n",
    "                # Candidate Items: [GT, Neg1, Neg2, ..., Neg100]\n",
    "                candidate_items = [gt_item] + negatives\n",
    "                candidate_tensor = torch.tensor(candidate_items, dtype=torch.long).to(device)\n",
    "                \n",
    "                # User Tensor: Repeat the user ID 101 times\n",
    "                user_tensor = torch.tensor([u_idx] * 101, dtype=torch.long).to(device)\n",
    "                \n",
    "                # 4. Score Items\n",
    "                scores = model(user_tensor, candidate_tensor)\n",
    "                scores = scores.cpu().numpy()\n",
    "                \n",
    "                # 5. Rank\n",
    "                # The Ground Truth is at index 0. We need to see where it lands.\n",
    "                # argsort gives indices that sort the array. \n",
    "                # We want descending sort.\n",
    "                ranked_indices = np.argsort(-scores) # \"-\" for descending\n",
    "                \n",
    "                # Find where the GT (index 0) ended up in the sorted list\n",
    "                # np.where returns a tuple, we take the first element\n",
    "                gt_rank = np.where(ranked_indices == 0)[0][0]\n",
    "                \n",
    "                # gt_rank is 0-indexed (0 means 1st place, 1 means 2nd place)\n",
    "                \n",
    "                # 6. Calculate Metrics per User\n",
    "                for k in self.k_list:\n",
    "                    # Hit Rate\n",
    "                    if gt_rank < k:\n",
    "                        hr_results[k].append(1)\n",
    "                        # NDCG: 1 / log2(rank + 2)\n",
    "                        # rank+2 because rank is 0-based. \n",
    "                        # If rank=0 (1st), log2(2)=1 -> NDCG=1.\n",
    "                        ndcg_results[k].append(1 / math.log2(gt_rank + 2))\n",
    "                    else:\n",
    "                        hr_results[k].append(0)\n",
    "                        ndcg_results[k].append(0)\n",
    "\n",
    "        # Average the results\n",
    "        avg_hr = {k: np.mean(v) for k, v in hr_results.items()}\n",
    "        avg_ndcg = {k: np.mean(v) for k, v in ndcg_results.items()}\n",
    "        \n",
    "        return avg_hr, avg_ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36cf969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Catalog Size\n",
    "# We need the total count to know the range for random sampling (1 to num_items)\n",
    "num_items_total = len(item2id)\n",
    "\n",
    "# 2. Create the Training Dataset\n",
    "# This will also pre-compute the 'exclusion_sets' (Train+Val+Test) for every user\n",
    "train_dataset = BPRDataset(\n",
    "    train_data=train_data, \n",
    "    val_data=val_data, \n",
    "    test_data=test_data, \n",
    "    num_items=num_items_total\n",
    ")\n",
    "\n",
    "# 3. Extract Exclusion Rules\n",
    "# We need these rules so the Evaluator doesn't accidentally sample \n",
    "# known positives (history) as \"negatives\" during the ranking test.\n",
    "exclusion_rules = train_dataset.exclusion_sets\n",
    "\n",
    "# 4. Create the Validation Evaluator\n",
    "# We use this to check HR@1 and HR@10 at the end of every epoch\n",
    "val_evaluator = BPREvaluator(\n",
    "    eval_data=val_data, \n",
    "    exclusion_rules=exclusion_rules, \n",
    "    num_items=num_items_total, \n",
    "    k_list=[1, 10]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e34116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device selected: cuda\n",
      "Starting Training...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 Completed\n",
      "------------------------------\n",
      "  Training Loss: 0.6931\n",
      "  Validation HR@1:   0.0256 | HR@10:   0.1495\n",
      "  Validation NDCG@1: 0.0256 | NDCG@10: 0.0769\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 Completed\n",
      "------------------------------\n",
      "  Training Loss: 0.6915\n",
      "  Validation HR@1:   0.0360 | HR@10:   0.1671\n",
      "  Validation NDCG@1: 0.0360 | NDCG@10: 0.0903\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 Completed\n",
      "------------------------------\n",
      "  Training Loss: 0.6858\n",
      "  Validation HR@1:   0.0471 | HR@10:   0.1747\n",
      "  Validation NDCG@1: 0.0471 | NDCG@10: 0.1006\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 Completed\n",
      "------------------------------\n",
      "  Training Loss: 0.6755\n",
      "  Validation HR@1:   0.0547 | HR@10:   0.1816\n",
      "  Validation NDCG@1: 0.0547 | NDCG@10: 0.1082\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 Completed\n",
      "------------------------------\n",
      "  Training Loss: 0.6605\n",
      "  Validation HR@1:   0.0613 | HR@10:   0.1893\n",
      "  Validation NDCG@1: 0.0613 | NDCG@10: 0.1159\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 Completed\n",
      "------------------------------\n",
      "  Training Loss: 0.6411\n",
      "  Validation HR@1:   0.0675 | HR@10:   0.1958\n",
      "  Validation NDCG@1: 0.0675 | NDCG@10: 0.1227\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 Completed\n",
      "------------------------------\n",
      "  Training Loss: 0.6191\n",
      "  Validation HR@1:   0.0730 | HR@10:   0.2028\n",
      "  Validation NDCG@1: 0.0730 | NDCG@10: 0.1290\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 Completed\n",
      "------------------------------\n",
      "  Training Loss: 0.5942\n",
      "  Validation HR@1:   0.0766 | HR@10:   0.2099\n",
      "  Validation NDCG@1: 0.0766 | NDCG@10: 0.1343\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09 Completed\n",
      "------------------------------\n",
      "  Training Loss: 0.5674\n",
      "  Validation HR@1:   0.0797 | HR@10:   0.2152\n",
      "  Validation NDCG@1: 0.0797 | NDCG@10: 0.1387\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Completed\n",
      "------------------------------\n",
      "  Training Loss: 0.5408\n",
      "  Validation HR@1:   0.0834 | HR@10:   0.2201\n",
      "  Validation NDCG@1: 0.0834 | NDCG@10: 0.1432\n",
      "============================================================\n",
      "\n",
      "Training Complete. Model saved to: /data/sukhanna/cse258/bpr_mf_model.pth\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "BATCH_SIZE = 128      # Reasonably small for 1 GPU\n",
    "LEARNING_RATE = 0.001 # Standard starting point for Adam\n",
    "NUM_EPOCHS = 10\n",
    "EMBEDDING_DIM = 64\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Device selected: {DEVICE}\")\n",
    "\n",
    "# --- Setup Model & Data ---\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model = BPRMF(num_users=len(train_data), num_items=len(item2id), embedding_dim=EMBEDDING_DIM)\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(\"Starting Training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    \n",
    "    # 1. Training Phase\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Tqdm progress bar for the batch loop\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\", leave=False)\n",
    "    \n",
    "    for user_ids, pos_items, neg_items in progress_bar:\n",
    "        # Move data to GPU\n",
    "        user_ids = user_ids.to(DEVICE)\n",
    "        pos_items = pos_items.to(DEVICE)\n",
    "        neg_items = neg_items.to(DEVICE)\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward Pass\n",
    "        # We calculate scores for (User, Positive) and (User, Negative)\n",
    "        pos_scores = model(user_ids, pos_items)\n",
    "        neg_scores = model(user_ids, neg_items)\n",
    "\n",
    "        # # Unpack data\n",
    "        # # neg_items shape: [Batch_Size, N_Neg]\n",
    "        # batch_size = user_ids.size(0)\n",
    "        # n_neg = neg_items.size(1) \n",
    "\n",
    "        # # 1. Flatten Negatives to fit into Model\n",
    "        # # We effectively treat these as (Batch * N) separate pairs\n",
    "        # flat_users = user_ids.repeat_interleave(n_neg) # Shape: [Batch * N]\n",
    "        # flat_negs = neg_items.view(-1)                 # Shape: [Batch * N]\n",
    "\n",
    "        # # 2. Compute Scores\n",
    "        # pos_scores = model(user_ids, pos_items)        # Shape: [Batch]\n",
    "        # flat_neg_scores = model(flat_users, flat_negs) # Shape: [Batch * N]\n",
    "\n",
    "        # # 3. Reshape Back for Loss\n",
    "        # neg_scores = flat_neg_scores.view(batch_size, n_neg) # Shape: [Batch, N]\n",
    "\n",
    "        # # 4. Calculate Loss with Broadcasting\n",
    "        # # pos_scores: [Batch] -> Unsqueeze to [Batch, 1] to broadcast against [Batch, N]\n",
    "        # loss = -torch.mean(torch.nn.functional.logsigmoid(pos_scores.unsqueeze(1) - neg_scores))\n",
    "\n",
    "\n",
    "        # BPR Loss Calculation\n",
    "        # Loss = - sum( log( sigmoid( pos_score - neg_score ) ) )\n",
    "        # We assume optimization minimizes loss, so we take negative log likelihood\n",
    "        loss = -torch.mean(torch.nn.functional.logsigmoid(pos_scores - neg_scores))\n",
    "        \n",
    "        # Backward Pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # 2. Evaluation Phase\n",
    "    # We evaluate on the Validation set at the end of every epoch\n",
    "    hr_metrics, ndcg_metrics = val_evaluator.evaluate(model, device=DEVICE)\n",
    "    \n",
    "    # 3. Reporting\n",
    "    print(f\"Epoch {epoch:02d} Completed\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"  Training Loss: {avg_loss:.4f}\")\n",
    "    print(f\"  Validation HR@1:   {hr_metrics[1]:.4f} | HR@10:   {hr_metrics[10]:.4f}\")\n",
    "    print(f\"  Validation NDCG@1: {ndcg_metrics[1]:.4f} | NDCG@10: {ndcg_metrics[10]:.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# --- Save Model ---\n",
    "save_path = \"/data/sukhanna/cse258/bpr_mf_model.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"\\nTraining Complete. Model saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a752cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test Evaluation...\n",
      "------------------------------\n",
      "Test HR@1:   0.0691 | HR@10:   0.2007\n",
      "Test NDCG@1: 0.0691 | NDCG@10: 0.1261\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. Instantiate the Test Evaluator\n",
    "# We use the SAME exclusion_rules from the training dataset.\n",
    "# This ensures we don't sample the Test Target (or Train/Val items) as negatives.\n",
    "test_evaluator = BPREvaluator(\n",
    "    eval_data=test_data, \n",
    "    exclusion_rules=train_dataset.exclusion_sets, \n",
    "    num_items=len(item2id), \n",
    "    k_list=[1, 10]\n",
    ")\n",
    "\n",
    "# 2. Run Evaluation\n",
    "print(\"Running Test Evaluation...\")\n",
    "test_hr, test_ndcg = test_evaluator.evaluate(model, device=DEVICE)\n",
    "\n",
    "# 3. Report Results\n",
    "print(\"-\" * 30)\n",
    "print(f\"Test HR@1:   {test_hr[1]:.4f} | HR@10:   {test_hr[10]:.4f}\")\n",
    "print(f\"Test NDCG@1: {test_ndcg[1]:.4f} | NDCG@10: {test_ndcg[10]:.4f}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e35109d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse258_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
